{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) What is the vanishing gradient problem in deep neural networks? How does it affect training?"
      ],
      "metadata": {
        "id": "bwxheU3y-_N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vanishing gradient problem in deep neural networks is a phenomenon where gradientsâ€”the values used to update neural network parametersâ€”become exceedingly small as they propagate back through the network's layers during training. This issue mainly affects very deep networks, where there are many layers between the input and the output. Hereâ€™s a closer look at what causes this problem, its effects on training, and possible solutions:\n",
        "\n",
        "---\n",
        "\n",
        "### **What Causes the Vanishing Gradient Problem?**\n",
        "\n",
        "When training deep neural networks, we use backpropagation to calculate gradients of the loss function with respect to each layerâ€™s parameters. These gradients are multiplied sequentially by the derivatives of each layer's activation function during backpropagation. In networks with many layers, this multiplication can cause gradients to shrink exponentially, especially when the activation functions have gradients less than 1 (like the sigmoid or tanh functions).\n",
        "\n",
        "For example:\n",
        "- **Sigmoid Activation**: The gradient of a sigmoid activation function is always between 0 and 0.25, meaning repeated multiplication in deep networks will yield very small values.\n",
        "- **Tanh Activation**: While centered around zero, it still has similar issues with gradient scaling, especially for large or small input values.\n",
        "\n",
        "### **Effects of the Vanishing Gradient Problem**\n",
        "\n",
        "The vanishing gradient problem significantly impacts the training of deep neural networks:\n",
        "\n",
        "1. **Slow Learning**: When gradients are tiny, they cause very small updates to the weights, which slows down learning. It may take an impractical amount of time for the network to learn anything meaningful.\n",
        "  \n",
        "2. **Poor Representation Learning in Early Layers**: Since gradients diminish as they move backward, the early (input-side) layers receive almost no gradient information. As a result, these layers fail to learn useful representations, leading to suboptimal model performance.\n",
        "\n",
        "3. **Difficulty in Training Deep Networks**: This problem makes it challenging to train very deep architectures because the effective learning happens only in the layers closer to the output, while early layers remain stagnant.\n",
        "\n",
        "### **Potential Solutions to the Vanishing Gradient Problem**\n",
        "\n",
        "Researchers have developed several techniques to mitigate this problem and allow deep networks to train effectively:\n",
        "\n",
        "1. **ReLU Activation Function**: ReLU (Rectified Linear Unit) and its variants (e.g., Leaky ReLU, ELU) are popular because they donâ€™t saturate like sigmoid or tanh, keeping gradients from diminishing as quickly. The ReLU gradient is 1 for positive inputs, which helps in maintaining larger gradients during backpropagation.\n",
        "\n",
        "2. **Batch Normalization**: This technique normalizes the inputs of each layer during training. By keeping inputs within a standard range, batch normalization reduces the risk of gradients vanishing and makes optimization more stable.\n",
        "\n",
        "3. **Weight Initialization Techniques**: Using specific weight initialization methods like Xavier or He initialization helps keep gradients within a manageable range, preventing them from becoming too small as they propagate through layers.\n",
        "\n",
        "4. **Residual Networks (ResNets)**: ResNets introduce shortcut (skip) connections that allow gradients to flow directly back to earlier layers without vanishing. This architecture enables networks to be trained with hundreds or even thousands of layers, overcoming the vanishing gradient problem effectively.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A6gbv6CU-_KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)  What are some common activation functions that are prone to causing vanishing gradients."
      ],
      "metadata": {
        "id": "gmirbkEG-_GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions that are prone to causing vanishing gradients are typically those that \"saturate,\" meaning they output values that are very close to a constant over a large range of input values. When the gradient is calculated in these regions, it becomes very small or even zero, which can make it difficult for a deep network to propagate useful gradients back to earlier layers. Some common activation functions prone to this issue include:\n",
        "\n",
        "### 1. **Sigmoid**\n",
        "   - **Output Range**: (0, 1)\n",
        "   - **Issue**: The sigmoid function saturates when inputs are large in magnitude (either positive or negative). In these regions, the gradient is close to zero, which can lead to vanishing gradients. This is especially problematic in deep networks where small gradients are repeatedly multiplied through multiple layers.\n",
        "   - **Example**: In deep networks, even small weight changes can result in neurons consistently outputting values near 0 or 1, limiting gradient flow and slowing or stalling training.\n",
        "\n",
        "### 2. **Hyperbolic Tangent (tanh)**\n",
        "   - **Output Range**: (-1, 1)\n",
        "   - **Issue**: While tanh has an advantage over sigmoid (its output is zero-centered), it still suffers from vanishing gradients in regions where the input is large (positive or negative). When the activation saturates at -1 or 1, the gradients become very small, resulting in similar issues as the sigmoid function.\n",
        "   - **Example**: Deep networks with tanh activations may still struggle with gradient flow, although less severely than with sigmoid, due to the zero-centered output.\n",
        "\n",
        "### 3. **Softmax (for intermediate layers)**\n",
        "   - **Output Range**: (0, 1), for each output neuron with sum of outputs equal to 1\n",
        "   - **Issue**: Though typically used in output layers for classification tasks, softmax can also be used in intermediate layers. When it is, gradients can diminish if the distribution is very skewed, especially when one class is highly confident compared to others.\n",
        "   - **Example**: If one output is much higher than others, softmax \"pushes\" this neuronâ€™s probability close to 1, leading to near-zero gradients for other neurons.\n",
        "\n",
        "### **Why Vanishing Gradient Problems Are Less Pronounced with ReLU and Variants**\n",
        "\n",
        "In contrast, activation functions like **ReLU (Rectified Linear Unit)** and its variants (e.g., **Leaky ReLU**, **Parametric ReLU (PReLU)**) are less prone to vanishing gradients because they do not saturate for positive inputs. ReLU outputs zero for negative inputs and a linear output for positive inputs, which allows gradients to pass through without significant reduction in most cases. This property is part of what has made ReLU and its variants so popular for training deep networks."
      ],
      "metadata": {
        "id": "G_xQe9I--_Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-WYzodcVBYzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Define the exploding gradient problem in deep neural networks. How does it impact training."
      ],
      "metadata": {
        "id": "TT1AcdGd---U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **exploding gradient problem** occurs in deep neural networks when gradients increase exponentially as they are propagated backward through many layers. This often happens in very deep networks or recurrent neural networks (RNNs), where small weight updates quickly become unmanageable. When gradients \"explode,\" they produce extremely large updates to the weights, causing the model to become unstable and fail to converge.\n",
        "\n",
        "### How the Exploding Gradient Problem Affects Training\n",
        "\n",
        "1. **Unstable Learning**: Large gradients lead to excessively large weight updates, which cause the loss to fluctuate wildly or even become undefined. This instability often prevents the network from converging, making it impossible to learn effectively.\n",
        "\n",
        "2. **NaN (Not a Number) Errors**: In severe cases, exploding gradients can lead to NaN values during training, as excessively large numbers are produced in the calculations, causing the network to fail entirely.\n",
        "\n",
        "3. **Poor Model Performance**: If training does manage to continue, the model may oscillate between poor solutions due to erratic weight updates. This often results in high error rates and poor generalization, as the network is unable to reach a stable solution.\n",
        "\n",
        "### Common Solutions to the Exploding Gradient Problem\n",
        "\n",
        "1. **Gradient Clipping**: One of the most effective ways to address exploding gradients is to clip them to a predefined maximum value. Gradient clipping limits the gradients within a certain threshold, preventing them from becoming too large. This technique is particularly useful in RNNs, where exploding gradients are more common.\n",
        "\n",
        "2. **Weight Regularization**: Techniques like L2 regularization add a penalty for large weights, which can help control the size of the gradients and mitigate the exploding gradient problem.\n",
        "\n",
        "3. **Careful Initialization**: Proper weight initialization, such as Xavier or He initialization, helps ensure that gradients remain within a reasonable range during training, reducing the chance of explosion.\n",
        "\n",
        "4. **Using Proper Activation Functions**: Activation functions with better gradient behavior, like ReLU and its variants, are less likely to contribute to exploding gradients than saturating functions like sigmoid or tanh in deep networks.\n",
        "\n",
        "5. **Adjusting Learning Rate**: A high learning rate can exacerbate the exploding gradient problem, as it increases the size of the updates. Reducing the learning rate can help stabilize the training process, especially in networks where large gradients are common."
      ],
      "metadata": {
        "id": "ShgcvWR9--ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "A7pbxTs1--sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What is the role of proper weight initialization in training deep neural networks."
      ],
      "metadata": {
        "id": "cBQRxGl3--kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proper **weight initialization** is crucial in training deep neural networks because it helps ensure that gradients flow correctly during backpropagation, preventing both vanishing and exploding gradient problems. Proper initialization improves convergence speed, stability, and the overall performance of the model.\n",
        "\n",
        "### Role of Weight Initialization in Training Deep Neural Networks\n",
        "\n",
        "1. **Maintaining Gradient Magnitude**: In deep networks, gradients need to be carefully balanced; if initial weights are too large, gradients can explode, and if they are too small, gradients can vanish. Proper initialization methods, like Xavier (Glorot) and He initialization, maintain the magnitude of gradients within a reasonable range, preserving the gradient flow through all layers.\n",
        "\n",
        "2. **Avoiding Symmetry**: When all weights are initialized with the same value, neurons in each layer learn the same features, resulting in a lack of diversity in learned representations. Randomized weight initialization breaks this symmetry, enabling different neurons to learn distinct features and improving model capacity.\n",
        "\n",
        "3. **Faster Convergence**: Well-initialized weights reduce the number of steps needed for the network to converge. Proper initialization helps the network start closer to a solution, making the learning process faster and more efficient.\n",
        "\n",
        "4. **Preventing Vanishing and Exploding Gradients**: Certain initialization methods, such as Xavier for sigmoid or tanh activations and He initialization for ReLU, are designed to maintain gradient magnitude at a scale appropriate for deep networks. This helps prevent gradients from vanishing or exploding as they propagate through layers.\n",
        "\n",
        "5. **Improving Generalization**: Properly initialized weights can lead to better optimization and, subsequently, better generalization to unseen data. Poorly initialized weights can lead to models getting stuck in poor local minima, whereas good initialization gives the model a better starting point to explore optimal solutions.\n",
        "\n",
        "### Common Initialization Methods and When to Use Them\n",
        "\n",
        "- **Xavier (Glorot) Initialization**: This method sets the initial weights based on the size of the previous and current layers. Itâ€™s often used for networks with sigmoid or tanh activations, as it maintains the variance of activations across layers.\n",
        "  \n",
        "- **He Initialization**: Designed for ReLU and its variants, He initialization scales weights according to the number of input units in a layer. This method helps networks with ReLU activations avoid vanishing gradients.\n",
        "\n",
        "- **LeCun Initialization**: Often used for SELU (Scaled Exponential Linear Unit) activations, it scales weights to suit layers with this particular activation, helping stabilize gradients during training.\n"
      ],
      "metadata": {
        "id": "YRld02p---gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Gl7uINtP--Zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)  Explain the concept of batch normalization and its impact on weight initialization techniques."
      ],
      "metadata": {
        "id": "OoxH9RmO--WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Concept of Batch Normalization**\n",
        "\n",
        "**Batch Normalization (BN)** is a technique used to improve the training of deep neural networks by normalizing the inputs to each layer within a mini-batch. This process helps address issues like internal covariate shift, which occurs when the distribution of inputs to a layer changes during training as the parameters of the previous layers are updated.\n",
        "\n",
        "#### **How Batch Normalization Works:**\n",
        "1. **Normalization**: For each mini-batch, BN normalizes the input features to have zero mean and unit variance. For a given layer with input \\( x \\), the normalized value is computed as:\n",
        "   \\[\n",
        "   \\hat{x} = \\frac{x - \\mu}{\\sigma}\n",
        "   \\]\n",
        "   where:\n",
        "   - \\( x \\) is the input,\n",
        "   - \\( \\mu \\) is the mean of the mini-batch,\n",
        "   - \\( \\sigma \\) is the standard deviation of the mini-batch.\n",
        "\n",
        "2. **Scaling and Shifting**: After normalization, BN introduces two learnable parameters, \\( \\gamma \\) (scale) and \\( \\beta \\) (shift), to maintain the network's capacity to represent any distribution, similar to what the original unnormalized data would represent. This step transforms the normalized values:\n",
        "   \\[\n",
        "   y = \\gamma \\hat{x} + \\beta\n",
        "   \\]\n",
        "   This allows the network to restore the ability to represent a broader range of data distributions.\n",
        "\n",
        "#### **Impact of Batch Normalization on Weight Initialization**\n",
        "\n",
        "Batch normalization influences weight initialization in the following ways:\n",
        "\n",
        "1. **Reduction in Sensitivity to Weight Initialization**:\n",
        "   - **Without Batch Normalization**: Weight initialization plays a critical role in determining how well gradients propagate through the network. Improper weight initialization can lead to vanishing or exploding gradients.\n",
        "   - **With Batch Normalization**: Batch normalization reduces the dependency on weight initialization because it normalizes the activations, ensuring that they are of a consistent scale across layers. This alleviates the issue of poor initialization that can lead to slow convergence or gradient-related problems (like vanishing/exploding gradients).\n",
        "\n",
        "2. **Easier Training with Higher Learning Rates**:\n",
        "   - Batch normalization makes the network less sensitive to the choice of learning rate, allowing for larger learning rates. Since it normalizes the input to each layer, it helps prevent the gradients from exploding, which can occur with larger learning rates, especially in deep networks.\n",
        "   - This means that you donâ€™t need to be as cautious with weight initialization, as BN helps stabilize the activations and gradients, allowing the network to converge more quickly.\n",
        "\n",
        "3. **Compatibility with Standard Initialization Methods**:\n",
        "   - **Without BN**: The choice of weight initialization methods, like Xavier or He initialization, plays a large role in ensuring proper gradient flow.\n",
        "   - **With BN**: Since BN normalizes activations, the network can tolerate a wider range of weight initializations. For example, **Xavier** initialization (which works well with sigmoid or tanh activations) or **He initialization** (better for ReLU) can still be effective, but BN can mitigate the problems that arise from poor initialization, allowing training to proceed even if the initial weights are not perfectly tuned.\n",
        "\n",
        "4. **Faster Convergence**:\n",
        "   - The normalization step ensures that the activations do not become too large or too small, which would otherwise slow down training or prevent learning altogether. As a result, batch normalization allows the network to converge faster and with less need for careful adjustment of initialization parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "OXfED1yO--QC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "y23ohPjh--MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Implement He initialization in Python using TensorFlow or PyTorch."
      ],
      "metadata": {
        "id": "atrgAyXH--Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He initialization, also known as He normal initialization, is a weight initialization method designed for layers with ReLU activation functions. It initializes the weights from a distribution with a mean of 0 and a variance of\n",
        "2\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "n\n",
        "in\n",
        "â€‹\n",
        "\n",
        "2\n",
        "â€‹\n",
        " , where\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "n\n",
        "in\n",
        "â€‹\n",
        "  is the number of input units to the layer. This helps prevent the vanishing gradient problem in deep networks with ReLU activations."
      ],
      "metadata": {
        "id": "MCESsChr--Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. He Initialization in TensorFlow\n",
        "In TensorFlow, He initialization can be done using the tf.keras.initializers.HeNormal or tf.keras.initializers.HeUniform."
      ],
      "metadata": {
        "id": "bk5-ch8-Cac6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a simple model with He initialization\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(), input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "q4myO1sNCSKc",
        "outputId": "d3cab695-e447-4278-828c-5b7244b430bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚         \u001b[38;5;34m100,480\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  â”‚           \u001b[38;5;34m1,290\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "The HeNormal() initializer draws the weights from a normal distribution with mean 0 and variance\n",
        "2\n",
        "ğ‘›\n",
        "ğ‘–\n",
        "ğ‘›\n",
        "n\n",
        "in\n",
        "â€‹\n",
        "\n",
        "2\n",
        "â€‹\n",
        " .\n",
        "The input_shape=(784,) is just an example, assuming you are working with 784 input features (e.g., flattened MNIST images)."
      ],
      "metadata": {
        "id": "eE9M_cMuCeCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. He Initialization in PyTorch\n",
        "In PyTorch, you can manually apply He initialization by setting the weights using torch.nn.init.kaiming_normal_ or torch.nn.init.kaiming_uniform_. Here's an example using kaiming_normal_:"
      ],
      "metadata": {
        "id": "GWN4FCIHChbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple neural network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 10)   # Second fully connected layer\n",
        "\n",
        "        # Apply He initialization to the layers\n",
        "        torch.nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Print model details\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYzaLNxJCjjC",
        "outputId": "c52d6f88-17ff-48c5-9fc8-5c66a50b9028"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleNN(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "The torch.nn.init.kaiming_normal_ initializes the weights with a normal distribution using He initialization.\n",
        "The nonlinearity='relu' parameter is crucial because it adjusts the scale of the initialization based on the ReLU activation function.\n",
        "The model consists of two fully connected layers: one with 784 input features and 128 output features, and another with 128 input features and 10 output features (for classification tasks like MNIST).\n",
        "Both implementations will initialize the weights of the first layer using He initialization, which helps to keep the activations in a good range for deep neural networks with ReLU activation functions."
      ],
      "metadata": {
        "id": "fxhYcGAACpVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END"
      ],
      "metadata": {
        "id": "eMojLFyi_AHr"
      }
    }
  ]
}